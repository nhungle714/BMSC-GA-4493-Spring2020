{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "### BMSC-GA 4493, BMIN-GA 3007 \n",
    "### Spring 2020\n",
    "### Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives**:\n",
    "\n",
    "1. Basic Math Revision.\n",
    "2. Introduction to Machine Learning.\n",
    "3. Logistic Regression Model.\n",
    "4. Multi-layer Perceptron Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction** \n",
    "\n",
    "1. If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex. See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues with writing equations. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "2. Upload and Submit your final jupyter notebook file in <a href='http://newclasses.nyu.edu '>newclasses.nyu.edu</a>\n",
    "\n",
    "3. Deadline: Thursday Feb 20th 2020 (3pm) **\n",
    "\n",
    "4. Questions and Clarification: <a href=\"https://piazza.com/nyumc.org/spring2020/bmscga4493andbminga3007/home\"> Class Piazza</a>\n",
    "\n",
    "5. ***HW submission instructions:*** Students should submit a zipped folder named netid_hwx where x is the hw number . The submission should consist of jupyter notebook with all the plots and expected outputs clearly visible in it. The zipped folder should also contain the data files. We should be able to run your ipynb without making directory changes. Not following the protocol might lead to deduction of scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1: Math and Machine Learning Revision (9 points)\n",
    "\n",
    "### Take derivatives of functions from 1.1 to 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. (1 point)\n",
    "\n",
    "$f(x) = e^{3x + 9}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. (1 point)\n",
    "\n",
    "$f(x) = \\sqrt{\\sum_{i=1}^{5}(a_i x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. (1 point)\n",
    "\n",
    "$f(x) = ln(2^x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. (1 point)\n",
    "\n",
    "$f(x) = ln(3^x * 9^x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. (1 point)\n",
    "\n",
    "$f(x) = ln(e^{2x +1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Assume we have two coins. Imagine the first coin is fair and has a probability distribution P, where P(head) = 0.5 and P(tail) = 0.5. Assume second coin is not fair and has a different distribution Q, where Q(head)=0.1 and Q(tail)=0.9. \n",
    "\n",
    "#### 1.6.a. What is entropy of P distribution? (1 point)\n",
    "#### 1.6.b. What is entropy of Q distribution? (1 point)\n",
    "#### 1.6.c. What is cross-entropy between P and Q i.e. CE(P,Q)? (1 point)\n",
    "#### 1.6.d. What is the cross-entropy between Q and P i.e. CE(Q,P)? (1 point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2: Solving Linear Regression via Mean Squared Error (MSE) Optimization Problem (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you have measured two variables X and Y, for a simple task, and you belive that they might be linearly related to each other. Here, our input X has 2 dimensions, and the output has 1 dimension. We will use super-script to indicate which sample it is, and sub-scipt to indicate which dimension it is. \n",
    "The measurements are as follows:\n",
    "\n",
    "###### (Training data D = {($X^1$, $Y^1$), ($X^2$, $Y^2$), ($X^3$, $Y^3$)})\n",
    "\n",
    "Sample 1: $X^1 = (x_1^1, x_2^1) = (2, 2)$,   $Y^1$ = 5\n",
    "\n",
    "Sample 2: $X^2 = (x_1^2, x_2^2) = (2, 7)$,   $Y^2$ = 3\n",
    "\n",
    "Sample 3: $X^3 = (x_1^3, x_2^3) = (-1, 0)$,   $Y^3$ = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the relationship between X and Y is linear, we can write this relationship as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = f_{W,B}(X) = WX + B = w_1*x_1 + w_2*x_2 + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $W = (w_1, w_2)$ and $B$ are the parameters of the model.\t\n",
    "We are interested in finding best values for W and B.\t\n",
    "We define 'best' in terms of a loss function between $f_{W,b}(X_i)$ and $Y_i$ for each ($X_i$ and $Y_i$) in the training data. \t\n",
    "Since $Y_i$s are real numbers, let's consider Mean Squared Error loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Mean Squared Error for this function, over training data, and W and B is:\n",
    "\n",
    "$MSELoss(D={(X_1, Y_1), (X_2, Y_2), (X_3, Y_3)}), W, B) = \\frac{1}{3}\\sum_{i=1}^{3} (f_{W,B}(X_i) - Y_i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. (6 points)\n",
    "Compute the partial derivative of $MESLoss(D, W, B)$, With respect to W and B.\t\n",
    "Remember that $X_1$, $X_2$, $X_3$, $Y_1$, $Y_2$, and $Y_3$ are constants, and already given to us as training data above.\n",
    "\n",
    "$\\frac{d}{d w_1} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d w_2} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d B} MSELoss(D, W, B) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. (6 points)\n",
    "Use matplotlib library and plot $\\frac{d}{d w1} MSELoss(D, W, B)$ for $w_1 = np.arange(0, 3, 0.5)$, when $w_2$ equals 1, and B equals to -5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "w1 = np.arange(0, 3, 0.5)\n",
    "# plot dMSELoss/dw1 here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. (6 points)\n",
    "What values of $w_1$, $w_2$ and $B$, make all partial derivatives zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. (6 points)\n",
    "If you start from an initial point $w_1^0 = 0.1$ , $w_2^0 = 0.1$ and $B^0 = 0.1$, and iteratively update your $w_1$, $w_2$, and B via gradient descent as follows:\n",
    "    \n",
    "$ w_1^{t+1} = w_1^t - 0.01 * \\frac{d}{d w_1} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "$ w_2^{t+1} = w_2^t - 0.01 * \\frac{d}{d w_2} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "$ B^{t+1} = B^t - 0.01 * \\frac{d}{d B} MSELoss(D, W, B) |_{w_1^t,w_2^t,B^t} $\t\n",
    "(Note: This is gradient descent with a 0.01 learning rate.)\n",
    "\n",
    "What are the values of Ws and B over iterations 0 to 50? (Don't compute by hand! Write a code!)\t\n",
    "Write a python script that computes these values for 50 iterations, i.e. lists of $\\{w_1^0, w^1_1,.., w_1^{50}\\}$, $\\{w_2^0, w_2^1,.., w_2^{50}\\}$, and $\\{B^0, B^1,.., B^{50}\\}$.\t\n",
    "Plot the lists of $w_1$s, $w_2$s and Bs over 50 iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. (6 points)\n",
    "Now that you learned the math and made the code yourself, we will use pytorch and automatic differentiation, to find optimal W and B!\t\n",
    "Again, consider data to be D = {($X_1$, $Y_1$), ($X_2$, $Y_2$), ($X_3$, $Y_3$)}) = {((2,2), 5), ((2,7), 3), ((-1,0), 1)}.\n",
    "\n",
    "Some of your steps are here. Fill in the rest and show a plot of the loss function, $w_1$, $w_2$ and B over these 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data X is: [(2, 2), (2, 7), (-1, 0)]\n",
      "data Y is: [5, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "D = [((2,2), 5), ((2,7), 3), ((-1,0), 1)]\n",
    "X = [d[0] for d in D]\n",
    "Y = [d[1] for d in D]\n",
    "print('data X is:', X)\n",
    "print('data Y is:', Y)\n",
    "\n",
    "model = torch.nn.Linear(2, 1, bias=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "losslist = []\n",
    "w1list = []\n",
    "w2list = []\n",
    "blist = []\n",
    "\n",
    "# for epoch in range(10):\n",
    "    # Shuffle your training data samples\n",
    "    # Loop over your training data in the new order:\n",
    "        #dont forget to: optimizer.zero_grad()\n",
    "        #prepare your x_input and y_target if needed\n",
    "        #send the data through your model: i.e. pred_i = model(x_input)\n",
    "        #send the prediction through the loss function too: i.e. lossout= loss(pred_i, y_target)\n",
    "        #call backward to back-propagate: i.e. lossout.backward()\n",
    "        #call optimizer.step() to update the model parameters based on the computed gradients\n",
    "        #keep the w1s, w2s, and bs, and loss value some list so you can plot them later\n",
    "\n",
    "#plot the losslist, w1s, w2s, and bs.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Learning Curves, Overfitting, and Machine Learning! (61 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to optimize, let's get some real machine learning done!\t\n",
    "\n",
    "Instead of the small dataset we had in questions 2 and 3, now let's use the the CBIS-DDSM (Curated Breast Imaging Subset of DDSM) dataset from <a href=\"https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM#385f2cd4e86f4142b1d32bdb5803bd96\"> here</a>\n",
    "\n",
    "\n",
    "In this homework, we will *only* focus on the following items in the dataset:\t\n",
    "Mass-Training-Description (csv)\t\n",
    "Mass-Test-Description (csv)\t\n",
    "(Don't download the images on your laptop! That file is too big and we deal with it on the cluster later!)\n",
    "\n",
    "This dataset contains several features related to Mammography and detection of breast cancer. \n",
    "\n",
    "The Mass-Training-Description and Mass-Test-Description include these columns:\n",
    "\n",
    "patient_id\t\n",
    "breast_density\t\n",
    "left or right breast\t\n",
    "image view\t\t\n",
    "abnormality id\t\t\n",
    "abnormality type\t\n",
    "mass shape\t\n",
    "mass margins\t\n",
    "assessment\t\n",
    "pathology\n",
    "\n",
    "There is more data in this dataset, including images, but for this homework we will not focus on them.\n",
    "\n",
    "We are interested in this question:\t\n",
    "Using variables:\t\n",
    "\n",
    "breast_density\t\n",
    "left or right breast\t\n",
    "image view\t\t\n",
    "abnormality id\t\t\n",
    "abnormality type\t\n",
    "mass shape\t\n",
    "mass margins\t\n",
    "\n",
    "How well can we predict the **pathology type**?\n",
    "\n",
    "We can answer that by training a model on the Mass-Training-Description, and evaluating it on Mass-Test-Description. \n",
    "See questions 3.1 and 3.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. (10 points)\n",
    "Write a script to prepare input [breast_density, left or right breast, image view, abnormality id,\n",
    "abnormality type, mass shape, mass margins] and output [pathology type].\n",
    "\n",
    "The output of your script should be a matrix X and a vector Y, where each row of X are one set of variables for a patient, and each row of Y is the pathology type class, for that patient.\t\n",
    "\n",
    "Use *matplotlib.imshow* to visualize the X.\t\n",
    "(And if there are multiple equivalent rows per patient, keep only one of them - any, up to you)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. (5 points)\n",
    "Repeat Question 3.1 for the test set - remember to make sure you have same number of columns for test and train set!\n",
    "Use matplotlib.imshow(xtest, aspect='auto') to show the x dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Logistic Regression (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.a.\n",
    "\n",
    "Design a multi-class logistic regression model which takes the input and outputs the probability of 3 classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.b.\n",
    "What are the sizes for your input and output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.c.\n",
    "What type of activation function you will use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.d.\n",
    "How many parameters you need to fit for your design?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Multi-layered-perceptron (15 points)\n",
    "\n",
    "Design a multi layer perceptron (MLP) with a single hidden layer which first maps the vectorized image to a vector of 100 then feeds this vector to a fully connected layer to get the probability of 3 classes.\n",
    "\n",
    "You can choose optimizer and criterion of interest.\n",
    "\n",
    "Plot the ***average loss on all the train samples*** per epoch. (Stop the training after 1000 epochs). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. (10 points)\n",
    "Add the loss of the test set to the loss of the train set and plot the ***average loss on all the test samples*** per epoch. (Stop the training after 100 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. (3 points)\n",
    "Define two types of activation functions you can use in the first layer. Which activation function you will use on the second fully connected layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. (3 points)\n",
    "How many parameters you need to fit for your design? How does adding another hidden layer effected the number of parameters to use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
